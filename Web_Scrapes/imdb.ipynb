{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick scrape to grab all revelant movie links from a search that is plugged into IMDB. Of all links on the page, it returns only those that are relvant to our search. \n",
    "##### This was used to further scrape data of individual movies to check for corrleations in features and box office results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing a single function to extract all title urls from an IMDB search\n",
    "def url_extractor(search_url):\n",
    "    #setting up initial BeautifulSoup object from websearch \n",
    "    init_resp = urllib.request.urlopen(search_url)\n",
    "    init_soup = BeautifulSoup(init_resp, 'html.parser')\n",
    "    #extract the number of films that the query returned. Used to confirm at end and generate each url \n",
    "    number_of_films = int(str(init_soup.find_all('div', class_='desc')[0].find_all('span')[0]).split(' ')[2].replace(',',''))\n",
    "    print(number_of_films)\n",
    "    #each page has 50 movies, so setting up a list to iterate through the pages, set up blank list to store final urls\n",
    "    iterative_urls = [i for i in range(1,number_of_films, 50)]\n",
    "    url_list = []\n",
    "    #loop through the 50-spaced interger values to generate entire list of needed search urls \n",
    "    for i in iterative_urls:\n",
    "        # set url\n",
    "        url = search_url + '&start=' + str(i)\n",
    "        #set up the BeautifulSoup object for this specific page of the search\n",
    "        resp = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(resp, 'html.parser')\n",
    "        #generating list of all links on this page\n",
    "        links = [a.get('href') for a in soup.find_all('a', href=True)]\n",
    "        # printing out where we are in the query to monitor efficiency\n",
    "        print('Running query from {} to {}'.format(i, i+49))\n",
    "        # checking each link in each search page for title/tt keyword, which identfies a potential link\n",
    "        for link in links:\n",
    "            if 'title/tt' in link:\n",
    "                #when the length is 4 of the split title url, that means it is part of query \n",
    "                #when the length is 3, it means that the movie is ancillary to the actual search (prequel/sequel)\n",
    "                if len(link.split('/')) == 4:\n",
    "                # format the resulting url in the correct manner and appending it to final list \n",
    "                    title_link = 'https://www.imdb.com' + '/' + link.split('/')[1] + '/' + link.split('/')[2] + '/?ref_=adv_li_i'\n",
    "                    if not title_link in url_list:\n",
    "                        url_list.append(title_link)\n",
    "                    else: \n",
    "                        continue \n",
    "                else: \n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    # Final test to make sure that the length of query equals the length of returned list and returning the final list\n",
    "    if len(url_list) == number_of_films:\n",
    "        print('All urls have been extracted successfully')\n",
    "    else: \n",
    "        print('WARNING: The number of films in this query was {}, but {} urls were returned'.format(number_of_films, len(url_list)))\n",
    "    return(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1720\n",
      "Running query from 1 to 50\n",
      "Running query from 51 to 100\n",
      "Running query from 101 to 150\n",
      "Running query from 151 to 200\n",
      "Running query from 201 to 250\n",
      "Running query from 251 to 300\n",
      "Running query from 301 to 350\n",
      "Running query from 351 to 400\n",
      "Running query from 401 to 450\n",
      "Running query from 451 to 500\n",
      "Running query from 501 to 550\n",
      "Running query from 551 to 600\n",
      "Running query from 601 to 650\n",
      "Running query from 651 to 700\n",
      "Running query from 701 to 750\n",
      "Running query from 751 to 800\n",
      "Running query from 801 to 850\n",
      "Running query from 851 to 900\n",
      "Running query from 901 to 950\n",
      "Running query from 951 to 1000\n",
      "Running query from 1001 to 1050\n",
      "Running query from 1051 to 1100\n",
      "Running query from 1101 to 1150\n",
      "Running query from 1151 to 1200\n",
      "Running query from 1201 to 1250\n",
      "Running query from 1251 to 1300\n",
      "Running query from 1301 to 1350\n",
      "Running query from 1351 to 1400\n",
      "Running query from 1401 to 1450\n",
      "Running query from 1451 to 1500\n",
      "Running query from 1501 to 1550\n",
      "Running query from 1551 to 1600\n",
      "Running query from 1601 to 1650\n",
      "Running query from 1651 to 1700\n",
      "Running query from 1701 to 1750\n",
      "All urls have been extracted successfully\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "test = url_extractor('https://www.imdb.com/search/title/?title_type=feature&release_date=2018-01-01,2018-03-01')\n",
    "end_time = datetime.datetime.now()\n",
    "time = (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 69 seconds to clean and extract 1720 urls.\n"
     ]
    }
   ],
   "source": [
    "print('It took {} seconds to clean and extract 1720 urls.'.format(time.seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
